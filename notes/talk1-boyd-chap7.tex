
\title{Talk 1: Distributed Optimization and Statistical Learning via ADMM (I)}
\date{}
\author{}

\maketitle

{\bfseries Main Resource: Chapter 7 of \cite{boyd2011distributed}}

\section{Recall of basic ADMM}

A general ADMM optimization problem is formulated as
\begin{align*}
    & \text{minimize} \quad f(x) + g(z) \\
    & \text{subject to} \quad Ax + Bz = c
\end{align*}
The augmented Lagrangian of this problem is given by
$$\mathcal{L}_{\rho}(x,z,y) = f(x) + g(z) + \langle y, Ax+Bz-c \rangle + \dfrac{1}{\rho}\lVert Ax+Bz-c \rVert^2.$$
The iterations are given by
\begin{align*}
    x^{k+1} & = \argmin_{x} \left\{ \mathcal{L}_{\rho}(x,z^k,y^k) \right\} \\
    z^{k+1} & = \argmin_{z} \left\{ \mathcal{L}_{\rho}(x^{k+1},z,y^k) \right\} \\
    y^{k+1} & = y^{k} + \rho (Ax^{k+1}+Bz^{k+1}-c)
\end{align*}

Convergence of ADMM: under the conditions
\begin{itemize}
    \item $f,g$ closed, proper convex;
    \item $\mathcal{L}_{\rho}(x,z,y)$ has a saddle point,
\end{itemize}
then as $k\rightarrow\infty$ one has
\begin{itemize}
    \item feasibility: $Ax^k + By^k - c \rightarrow 0$
    \item objective: $f(x^k) + g(y^k) \rightarrow p^*$
    \item dual: $y^k \rightarrow y^*$
\end{itemize}

\section{Consensus Problem}

Assume we have global variable $x \in \mathbb{R}^n$ and ``split'' (or distributed) objective function
$$f(x) = \sum\limits_{i=1}^N f_i(x)$$
e.g. $x$ can be (global) model parameters, DNN weights (and biases, etc.), $f_i$ can be the loss function associated with the $i$-th block of data. The optimization problem is
\begin{align*}
    & \text{minimize} \quad \sum\limits_{i=1}^N f_i(x)
\end{align*}

Problem: $f$ is NOT block-separable.

Solution: add a common global variable $z \in \mathbb{R}^n$, so that the optimization problem is (equivalently) formulated as
\begin{align*}
    & \text{minimize} \quad \sum\limits_{i=1}^N f_i(x_i) \\
    & \text{subject to} \quad x_i - z = 0, \quad i=1,\cdots,N
\end{align*}
which is called a {\bfseries consensus problem}. One has the augmented Lagrangian
$$\mathcal{L}_{\rho}(x_1,\cdots,x_N,z,y) = \sum\limits_{i=1}^N \left[ f_i(x_i) + \langle y_i, x_i-z \rangle + \dfrac{\rho}{2} \lVert x_i-z \rVert^2 \right],$$
and iterations
\begin{align*}
    \phantom{\leadsto} & \left( x = (x_1^T,\cdots,x_N^T)^T, x^{k+1} = \argmin_x \left\{ \sum\limits_{i=1}^N \left[ f_i(x_i) + \langle y^k_i, x_i-z^k \rangle + \dfrac{\rho}{2} \lVert x_i-z^k \rVert^2 \right] \right\} \right) \\
    \leadsto \quad & x_i^{k+1} = \argmin_{x_i} \left\{ f_i(x_i) + \langle y^k_i, x_i-z^k \rangle + \dfrac{\rho}{2} \lVert x_i-z^k \rVert^2 \right\} \\
    & z^{k+1} = \argmin_{z} \left\{ \sum\limits_{i=1}^N \left[ f_i(x_i^{k+1}) + \langle y^k_i, x_i^{k+1}-z \rangle + \dfrac{\rho}{2} \lVert x_i^{k+1}-z \rVert^2 \right] \right\} \\
    & \phantom{z^{k+1}} = \argmin_{z} \left\{ \dfrac{N\rho}{2} \lVert z \rVert^2 - \langle z, \sum\limits_{i=1}^N (y_i^k+\rho x_i^{k+1}) \rangle + \cdots \right\} \\
    & \phantom{z^{k+1}} = \dfrac{1}{N} \sum\limits_{i=1}^N \left( \dfrac{y_i^{k}}{\rho} + x_i^{k+1} \right) \\
    & y_i^{k+1} = y_i^k + \rho (x_i^{k+1} - z^{k+1})
\end{align*}

Simplify notations by letting
$$
\begin{cases}
\overline{x}^k := \dfrac{1}{N} \sum\limits_{i=1}^N x_i^k \\
\overline{y}^k := \dfrac{1}{N} \sum\limits_{i=1}^N y_i^k \\
\end{cases}
$$
then one has the following observations
\begin{itemize}
    \item $$\displaystyle z^{k+1} = \dfrac{1}{N\rho} \sum\limits_{i=1}^N y_i^k + \dfrac{1}{N} \sum\limits_{i=1}^N x_i^{k+1} = \dfrac{\overline{y}^k}{\rho} + \overline{x}^{k+1}$$
    \item \begin{align*}
        & y^{k+1}_i = y_i^k + \rho(x_i^{k+1}-z^{k+1}) = y_i^k + \rho(x_i^{k+1}-\dfrac{\overline{y}^k}{\rho} - \overline{x}^{k+1}) = 0 \\
        \Rightarrow & \overline{z}^{k+1} = \dfrac{0}{\rho} + \overline{x}^{k+1} = \overline{x}^{k+1} \\
    \end{align*}
\end{itemize}
Then one can rewrite the iterations as
$$
\begin{cases}
x^{k+1}_i = \argmin\limits_{x_i} \left\{ f_i(x_i) + \langle y_i^k, x_i-\overline{x}^k \rangle + \dfrac{\rho}{2}\lVert x_i-\overline{x}^k \rVert^2 \right\} \\
(z^{k+1} = \overline{x}^{k+1}) \\
y_i^{k+1} = y_i^k + \rho(x^{k+1}_i-\overline{x}^{k+1})
\end{cases}
$$
This can be further simplified by setting $u_i = \dfrac{y_i}{\rho}$:
$$
\begin{cases}
x^{k+1}_i = \argmin\limits_{x_i} \left\{ f_i(x_i) + \dfrac{\rho}{2}\lVert x_i-\overline{x}^k + u_i^k \rVert^2 \right\} = \operatorname{prox}_{f_i,\rho}(\overline{x}^k - u_i^k) \\
(z^{k+1} = \overline{x}^{k+1}) \\
u_i^{k+1} = u_i^k + (x^{k+1}_i-\overline{x}^{k+1})
\end{cases}
$$

{\bfseries Statistical Interpretation} for the ADMM iterations for consensus problem: at iteration $k+1$, assume $x_i$ has prior distribution
$$x_i \sim N(\overline{x}^k-u_i^k, \rho I_n)$$
or equivalently
$$p(x_i) = \det(2\pi\rho I)^{-1/2} \exp\left(-\dfrac{1}{2} \right)$$

\bibliographystyle{plain}
\bibliography{references}
